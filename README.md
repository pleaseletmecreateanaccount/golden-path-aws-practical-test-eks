# ðŸ›¤ï¸ Golden Path â€” EKS Platform on AWS

> **Practical Test Submission** â€” Platform Engineer (AWS)


## Architecture Overview

![Golden Path EKS High Level Architecture](EKS-Golden-Path-Diagram.png)

## Repository Structure

```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy.yml          # CI/CD: Plan â†’ Apply â†’ Helm deploy
â”‚       â””â”€â”€ destroy.yml         # Cost-saver: Tear everything down safely
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ bootstrap/
â”‚   â”‚   â”œâ”€â”€ main.tf             # S3 + DynamoDB for remote state (run once)
â”‚   â”‚   â””â”€â”€ github-oidc.tf      # GitHub Actions IAM role (OIDC, no static keys)
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ vpc/                # VPC, subnets, NAT GWs, route tables
â”‚   â”‚   â”œâ”€â”€ eks/                # EKS control plane, Spot + On-Demand node groups
â”‚   â”‚   â””â”€â”€ irsa/               # IAM Roles for Service Accounts (app, ALB, ESO)
â”‚   â””â”€â”€ environments/
â”‚       â””â”€â”€ production/
â”‚           â”œâ”€â”€ main.tf         # Orchestrates all modules + Helm providers
â”‚           â”œâ”€â”€ variables.tf
â”‚           â”œâ”€â”€ outputs.tf
â”‚           â”œâ”€â”€ providers.tf
â”‚           â”œâ”€â”€ backend.tf      # S3 remote state
â”‚           â””â”€â”€ cloudwatch-dashboard.json.tpl
â”‚
â”œâ”€â”€ helm/
â”‚   â””â”€â”€ golden-path/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ values.yaml         # Defaults (image, HPA, ALB, ExternalSecret)
â”‚       â””â”€â”€ templates/
â”‚           â”œâ”€â”€ _helpers.tpl
â”‚           â”œâ”€â”€ deployment.yaml # Deployment with Spot affinity + fallback
â”‚           â””â”€â”€ resources.yaml  # Service, Ingress, HPA, SA, ExternalSecret, PDB
â”‚
â””â”€â”€ README.md
```

---


## How to Run

### Step 1 â€” Bootstrap (run once)

This creates the S3 bucket and DynamoDB table for Terraform remote state, and the GitHub Actions IAM role.

```bash
cd terraform/bootstrap

# Use temporary credentials or your personal AWS credentials here
aws configure

terraform init
terraform apply

# Note the outputs:
# github_actions_role_arn
# s3_bucket_name
```

### Step 2 â€” Configure GitHub Secrets & Variables

In your GitHub repository (`Settings â†’ Secrets and variables â†’ Actions`):

**Secrets:**
| Name | Value |
|------|-------|
| `AWS_ROLE_ARN` | Output from bootstrap: `github_actions_role_arn` |
| `DB_PASSWORD` | A strong password for the demo DB secret |

**Variables:**
| Name | Value |
|------|-------|
| `AWS_REGION` | `` |
| `AWS_ACCOUNT_ID` | `` |

### Step 3 â€” Deploy via GitHub Actions

Push to `main` â€” the deploy workflow runs automatically:

1. **Terraform Plan** â€” shows what will be created
2. **Terraform Apply** â€” provisions VPC, EKS, IAM, Secrets Manager, etc.
3. **Helm Deploy** â€” deploys nginx with ALB, HPA, ExternalSecret

Or trigger manually:
```
GitHub â†’ Actions â†’ "Deploy â€” Infrastructure & App" â†’ Run workflow
```

### Step 4 â€” Access the Application

After the workflow succeeds, get the ALB hostname:

```bash
aws eks update-kubeconfig --region ap-southeast-1 --name golden-path-production

kubectl get ingress -n golden-path
# NAME                  CLASS   HOSTS   ADDRESS                              PORTS
# golden-path-golden-path   alb   *       k8s-goldenpath-xxx.ap-southeast-1.elb.amazonaws.com   80
```

Visit `http://<ADDRESS>` in your browser â€” you'll see the nginx welcome page.

### Step 5 â€” Destroy (cost-saver)

When you're done testing, tear everything down:

```
GitHub â†’ Actions â†’ "Destroy â€” Tear Down All Resources" â†’ Run workflow
```

Type `DESTROY` in the confirmation field. The workflow will:
1. Uninstall the Helm release (removes ALB)
2. Run `terraform destroy`

> **Note:** The Terraform state S3 bucket is preserved. Delete it manually if desired.

---

## Key Design Decisions

### EKS (Managed Node Groups) vs ECS Fargate

| Factor | EKS | ECS Fargate |
|--------|-------------|-------------|
| **Kubernetes ecosystem** | Full â€” HPA, RBAC, Helm, ExternalSecrets, custom controllers | Limited â€” no standard K8s tooling |
| **Portability** | Workloads portable to any K8s cluster | AWS-locked |
| **Control** | Fine-grained scheduling, affinity, taints | Abstracted away |
| **Cost at scale** | Spot nodes = 60-90% savings | Per-task pricing, no Spot equivalent |
| **Complexity** | Higher operational overhead | Simpler to start |
| **GitOps readiness** | Argo CD, Flux, Helm-native | Limited |

**Decision:** EKS with managed node groups was chosen because this is a "Golden Path" for a microservices platform â€” developers will expect standard Kubernetes primitives, Helm charts, and ecosystem tools (ESO, ALB controller, HPA). The platform team owns the node groups; developers get a clean namespace and a Helm chart.

Fargate would be preferable for a smaller team that wants zero node management and is comfortable with ECS task definitions rather than Kubernetes manifests.

### Spot + On-Demand Fallback Strategy

- **Spot node group** (primary): 3 diverse instance types (`t3.medium`, `t3a.medium`, `t2.medium`) maximise Spot pool availability. Scales 0â€“10 nodes.
- **On-Demand node group** (fallback): Always keeps â‰¥1 node for critical system pods. Has a `node-type=on-demand:NoSchedule` taint.
- **App pods**: `preferredDuringSchedulingIgnoredDuringExecution` affinity prefers Spot (weight 80) but the On-Demand toleration allows fallback if Spot capacity is interrupted.
- **PodDisruptionBudget** ensures at least 1 replica survives node drains during Spot interruptions.

### Secret Management (External Secrets Operator)

```
AWS Secrets Manager â”€â”€â–º External Secrets Operator (IRSA) â”€â”€â–º Kubernetes Secret â”€â”€â–º Pod env var
     (source of truth)           (sync engine)                  (auto-refreshed 1h)
```

No secret values ever appear in Helm values, Git, or environment variables. The ESO pod uses IRSA (OIDC-federated IAM role) to call Secrets Manager â€” no static AWS credentials anywhere.

### IRSA (IAM Roles for Service Accounts)

Three distinct IAM roles are created via OIDC federation:
1. **App role** â€” S3 read/write on `golden-path-app-data-*`
2. **ALB Controller role** â€” EC2/ELB permissions to manage load balancers
3. **External Secrets role** â€” SecretsManager read on `golden-path/*`

Each role's trust policy is scoped to a specific Kubernetes namespace + service account, following least-privilege.

### CloudWatch Dashboard â€” 4 Golden Signals

The dashboard at `CloudWatch â†’ Dashboards â†’ golden-path-production-golden-signals` tracks:

| Signal | Metric Source |
|--------|--------------|
| **Latency** | ALB `TargetResponseTime` (p50/p95/p99) |
| **Traffic** | ALB `RequestCount` + Container Insights pod count |
| **Errors** | ALB `HTTPCode_Target_4XX_Count` + `5XX_Count` + container restarts |
| **Saturation** | Container Insights `pod_cpu_utilization`, `pod_memory_utilization`, `node_cpu_utilization` |

An annotation on the CPU saturation graph marks the HPA threshold (70%) so the team can immediately see when scaling events will be triggered.

---

## Local Development / Manual Terraform

```bash
cd terraform/environments/production

# Authenticate
aws sso login  # or: export AWS_PROFILE=your-profile

terraform init
terraform plan -var="db_password=my-test-password"
terraform apply -var="db_password=my-test-password"

# Update kubeconfig
aws eks update-kubeconfig --region ap-southeast-1 --name golden-path-production

# Deploy the app manually
helm upgrade --install golden-path ./helm/golden-path \
  --namespace golden-path --create-namespace \
  --set "serviceAccount.annotations.eks\.amazonaws\.com/role-arn=$(terraform output -raw app_irsa_role_arn)"
```

---

> Use the **Destroy workflow** to reduce cost to ~$0 between tests. The NAT GW is the biggest cost; reduce to 1 AZ in dev to save ~$67.
